{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b5f729c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\petra\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\paddle\\utils\\cpp_extension\\extension_utils.py:718: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running verify PaddlePaddle program ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\petra\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\paddle\\pir\\math_op_patch.py:219: UserWarning: Value do not have 'place' interface for pir graph mode, try not to use it. None will be returned.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PaddlePaddle works well on 1 GPU.\n",
      "PaddlePaddle is installed successfully! Let's start deep learning with PaddlePaddle now.\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "paddle.utils.run_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b694bf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.2.1+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    \n",
    "    x = torch.randn(100, 100).cuda()\n",
    "    y = torch.randn(100, 100).cuda()\n",
    "    z = torch.matmul(x, y)\n",
    "    print(f\"GPU вычисления работают: {z.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "792e5914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\petra\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LlamaFlashAttention2' from 'transformers.models.llama.modeling_llama' (c:\\Users\\petra\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m model_name = \u001b[33m'\u001b[39m\u001b[33mdeepseek-ai/DeepSeek-OCR\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      7\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m model = \u001b[43mAutoModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_attn_implementation\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mflash_attention_2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m model = model.eval().cuda().to(torch.bfloat16)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# prompt = \"<image>\\nFree OCR. \"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\petra\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:549\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mquantization_config\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    547\u001b[39m     _ = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mquantization_config\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m config, kwargs = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig.get(\u001b[33m\"\u001b[39m\u001b[33mtorch_dtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\petra\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:1346\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1341\u001b[39m     trust_remote_code = resolve_trust_remote_code(\n\u001b[32m   1342\u001b[39m         trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code, upstream_repo\n\u001b[32m   1343\u001b[39m     )\n\u001b[32m   1345\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[32m-> \u001b[39m\u001b[32m1346\u001b[39m     config_class = \u001b[43mget_class_from_dynamic_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1347\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1348\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1349\u001b[39m     config_class.register_for_auto_class()\n\u001b[32m   1350\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class.from_pretrained(pretrained_model_name_or_path, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\petra\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\dynamic_module_utils.py:616\u001b[39m, in \u001b[36mget_class_from_dynamic_module\u001b[39m\u001b[34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001b[39m\n\u001b[32m    603\u001b[39m \u001b[38;5;66;03m# And lastly we get the class inside our newly created module\u001b[39;00m\n\u001b[32m    604\u001b[39m final_module = get_cached_module_file(\n\u001b[32m    605\u001b[39m     repo_id,\n\u001b[32m    606\u001b[39m     module_file + \u001b[33m\"\u001b[39m\u001b[33m.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    614\u001b[39m     repo_type=repo_type,\n\u001b[32m    615\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_class_in_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\petra\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\dynamic_module_utils.py:311\u001b[39m, in \u001b[36mget_class_in_module\u001b[39m\u001b[34m(class_name, module_path, force_reload)\u001b[39m\n\u001b[32m    309\u001b[39m \u001b[38;5;66;03m# reload in both cases, unless the module is already imported and the hash hits\u001b[39;00m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[33m\"\u001b[39m\u001b[33m__transformers_module_hash__\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) != module_hash:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[43mmodule_spec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m     module.__transformers_module_hash__ = module_hash\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, class_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:994\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:488\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.cache\\huggingface\\modules\\transformers_modules\\deepseek_hyphen_ai\\DeepSeek_hyphen_OCR\\9f30c71f441d010e5429c532364a86705536c53a\\modeling_deepseekocr.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_deepseekv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeepseekV2Model, DeepseekV2ForCausalLM\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration_deepseek_v2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeepseekV2Config\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_outputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseModelOutputWithPast, CausalLMOutputWithPast\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.cache\\huggingface\\modules\\transformers_modules\\deepseek_hyphen_ai\\DeepSeek_hyphen_OCR\\9f30c71f441d010e5429c532364a86705536c53a\\modeling_deepseekv2.py:37\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcache_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Cache, DynamicCache\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_attn_mask_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _prepare_4d_causal_attention_mask\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllama\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_llama\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     38\u001b[39m     LlamaAttention,\n\u001b[32m     39\u001b[39m     LlamaFlashAttention2\n\u001b[32m     40\u001b[39m )\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_outputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     42\u001b[39m     BaseModelOutputWithPast,\n\u001b[32m     43\u001b[39m     CausalLMOutputWithPast,\n\u001b[32m     44\u001b[39m     SequenceClassifierOutputWithPast,\n\u001b[32m     45\u001b[39m )\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'LlamaFlashAttention2' from 'transformers.models.llama.modeling_llama' (c:\\Users\\petra\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py)"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "model_name = 'deepseek-ai/DeepSeek-OCR'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, _attn_implementation='flash_attention_2', trust_remote_code=True, use_safetensors=True)\n",
    "model = model.eval().cuda().to(torch.bfloat16)\n",
    "\n",
    "# prompt = \"<image>\\nFree OCR. \"\n",
    "prompt = \"<image>\\n<|grounding|>Convert the document to markdown. \"\n",
    "image_file = '6.jpg'\n",
    "output_path = 'output'\n",
    "\n",
    "# infer(self, tokenizer, prompt='', image_file='', output_path = ' ', base_size = 1024, image_size = 640, crop_mode = True, test_compress = False, save_results = False):\n",
    "\n",
    "# Tiny: base_size = 512, image_size = 512, crop_mode = False\n",
    "# Small: base_size = 640, image_size = 640, crop_mode = False\n",
    "# Base: base_size = 1024, image_size = 1024, crop_mode = False\n",
    "# Large: base_size = 1280, image_size = 1280, crop_mode = False\n",
    "\n",
    "# Gundam: base_size = 1024, image_size = 640, crop_mode = True\n",
    "\n",
    "res = model.infer(tokenizer, prompt=prompt, image_file=image_file, output_path = output_path, base_size = 1024, image_size = 640, crop_mode=True, save_results = True, test_compress = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c39aeba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'gen-1769501012-yv7SiZAMYBpkB7VHTlzP', 'provider': 'Google AI Studio', 'model': 'google/gemma-3-4b-it:free', 'object': 'chat.completion', 'created': 1769501012, 'choices': [{'logprobs': None, 'finish_reason': 'stop', 'native_finish_reason': 'STOP', 'index': 0, 'message': {'role': 'assistant', 'content': \"The image shows two dolphins leaping out of the water! They appear to be interacting with each other, possibly playing or communicating. \\n\\nHere's a breakdown of what's visible:\\n\\n*   **Dolphins:** Two gray dolphins with a distinctive white stripe on their sides.\\n*   **Water:** The dolphins are surrounded by blue water with ripples and waves created by their movement.\\n*   **Foam:** White foam is splashing around them as they jump.\\n\\nIt's a beautiful and dynamic shot of these marine mammals!\", 'refusal': None, 'reasoning': None, 'annotations': []}}], 'usage': {'prompt_tokens': 265, 'completion_tokens': 0, 'total_tokens': 265, 'cost': 0, 'is_byok': False, 'prompt_tokens_details': {'cached_tokens': 0, 'cache_write_tokens': 0, 'audio_tokens': 0, 'video_tokens': 0}, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'image_tokens': 0}}}\n"
     ]
    }
   ],
   "source": [
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b146b6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR2. This is not supported for all configurations of models and can yield errors.\n",
      "C:\\Users\\petra\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "BASE:  torch.Size([1, 256, 1280])\n",
      "PATCHES:  torch.Size([6, 144, 1280])\n",
      "=====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|ref|>text<|/ref|><|det|>[[30, 58, 710, 103]]<|/det|>\n",
      "Общество с ограниченной ответственностью \"Автодетальснаб 3160\", ИНН 7327028230, Россия, 432045, Ульяновская обл., г.Ульяновск, Московское шоссе, д.17, тел.: 34-84-04, р/с 40702810669020107395, в банке ОТДЕЛЕНИЕ № 8588 СБЕРБАНКА РОССИИ Г.УЛЬЯНОВСК, БИК 047308602, к/с 30101810000000000602 \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[789, 60, 920, 80]]<|/det|>\n",
      "Форма по ОКУД 0330212 \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[789, 88, 920, 108]]<|/det|>\n",
      "по ОКПО 25446636 \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[321, 108, 561, 127]]<|/det|>\n",
      "организация-грузоотправитель, адрес, телефон, факс, банковские реквизиты \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[40, 150, 760, 179]]<|/det|>\n",
      "Грузополучатель Легков Дмитрий Александрович, Россия, 155334, Ивановская обл., г. Вичуга, ул. Коровина, дом № 7а, кв. 10, тел.: 8-905-106-41-16, LD37@mail.ru \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[40, 197, 760, 226]]<|/det|>\n",
      "Общество с ограниченной ответственностью \"АвтодетальснаБ 3160\", ИНН 7327028230, Россия, 432045,Ульяновская обл., г.Ульяновск, Московское шоссе, \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[40, 222, 760, 243]]<|/det|>\n",
      "Поставщик 30101810000000000602, в банке ОТДЕЛЕНИЕ № 8588 СБЕРБА НКА РОССИИ Г.УЛЬЯНОВСК, БИК 04 7308602, к/с \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[40, 240, 920, 260]]<|/det|>\n",
      "Поставщик 30101810000000000602, в банке СТЕЛЕНИЕ № 8588 СБЕРБАНКА РОССИИ, г.Ульяновск, Московское шоссе, \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[39, 256, 920, 277]]<|/det|>\n",
      "Плательщик Легков Дмитрий Александрович, Россия, 155334, Ивановска обл., г. Вичуга, ул. Коровина, дом № 1а, кв. 10, тел.: 8-905-106-41.16, LD37@mail.ru \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[39, 274, 920, 293]]<|/det|>\n",
      "Ознавание Счет №3645 от 09.10.2015 \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[40, 302, 920, 320]]<|/det|>\n",
      "Номер документа 2820\n",
      "Дата составления 15.10.2015 \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[40, 316, 920, 336]]<|/det|>\n",
      "Номер документа 2820\n",
      "Дата составления 15.10. 2015 \n",
      "\n",
      "<|ref|>sub_title<|/ref|><|det|>[[213, 336, 362, 357]]<|/det|>\n",
      "ТОВАРНАЯ НАКЛАДНАЯ \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[380, 305, 490, 320]]<|/det|>\n",
      "договор, заказ-наряд \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[365, 321, 520, 337]]<|/det|>\n",
      "Номер документа 2820 \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[365, 336, 520, 352]]<|/det|>\n",
      "Дата составления 15.10.2015 \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[0, 0, 999, 0]]<|/det|>\n",
      "Страница 1 \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[0, 0, 999, 0]]<|/det|>\n",
      " \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[0, 0, 999, 0]]<|/det|>\n",
      "Номер документа 2820\n",
      "Дата составления 15.10.20 15 \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[0, 0, 999, 0]]<|/det|>\n",
      "Ознавание Счет №3645 от 09.10.2015\n",
      "===============save results:===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image: 0it [00:00, ?it/s]\n",
      "other: 100%|██████████| 20/20 [00:00<00:00, 83970.05it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "model_name = 'deepseek-ai/DeepSeek-OCR-2'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, _attn_implementation='flash_attention_2', trust_remote_code=True, use_safetensors=True)\n",
    "model = model.eval().cuda().to(torch.bfloat16)\n",
    "\n",
    "# prompt = \"<image>\\nFree OCR. \"\n",
    "prompt = \"<image>\\n<|grounding|>Convert the document to markdown. \"\n",
    "image_file = r'dataset\\TovarnayaNakladnaya\\1f9cafas-1920.jpg'\n",
    "output_path = 'your/output/dir'\n",
    "\n",
    "res = model.infer(tokenizer, prompt=prompt, image_file=image_file, output_path = output_path, base_size = 1024, image_size = 768, crop_mode=True, save_results = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be80bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exists: False\n",
      "abs: c:\\Users\\petra\\OneDrive\\Desktop\\test\\dataset\\TovarnayaNakladnaya\u0001f9cafas-1920.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"exists:\", os.path.exists(image_file))\n",
    "print(\"abs:\", os.path.abspath(image_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f780d070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 5060 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
